{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syunar/paper_replication/blob/main/PatchTST/patchtst.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "TcItClEpvyV9",
        "outputId": "7460344f-47bc-46cc-8716-4de1495546f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell\n",
        "from typing import Callable, Optional\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "id": "eVvSK83TrCya"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configs"
      ],
      "metadata": {
        "id": "zUfsv1Ie05Nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config():\n",
        "    random_seed = 42\n",
        "    is_training = 1\n",
        "    data_path = \"/content/all_six_datasets/weather\"\n",
        "    model = \"PatchTST\"\n",
        "    data = \"custom\"\n",
        "    features = 'M'\n",
        "    seq_len = 336\n",
        "    pred_len = 96\n",
        "    enc_in = 21\n",
        "    e_layers = 3\n",
        "    n_heads = 16\n",
        "    d_model = 128\n",
        "    d_ff = 256\n",
        "    dropout = 0.2\n",
        "    fc_dropout = 0.2\n",
        "    head_dropout = 0\n",
        "    patch_len = 16\n",
        "    stride = 8\n",
        "    des = 'Exp'\n",
        "    train_epochs = 100\n",
        "    patience = 20\n",
        "    itr = 1\n",
        "    batch_size = 128\n",
        "    learning_rate = 0.0001\n",
        "\n",
        "\n",
        "    individual = 0\n",
        "    padding_patch = 'end'\n",
        "    revin = 1 # True or False\n",
        "    affine = 0 # True or False\n",
        "    subtract_last = 0 # '0: subtract mean; 1: subtract last'\n",
        "    decomposition = 0 # True or False\n",
        "    kernel_size = 25 # for moving average when do decomposition\n",
        "\n",
        "args = Config()"
      ],
      "metadata": {
        "id": "FoEgk6aV06I_"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decomposition"
      ],
      "metadata": {
        "id": "LnQ1UMKbuOku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "class Transpose(nn.Module):\n",
        "    def __init__(self, *dims, contiguous=False):\n",
        "        super().__init__()\n",
        "        self.dims, self.contiguous = dims, contiguous\n",
        "    def forward(self, x):\n",
        "        if self.contiguous: return x.transpose(*self.dims).contiguous()\n",
        "        else: return x.transpose(*self.dims)\n",
        "\n",
        "\n",
        "def get_activation_fn(activation):\n",
        "    if callable(activation): return activation()\n",
        "    elif activation.lower() == \"relu\": return nn.ReLU()\n",
        "    elif activation.lower() == \"gelu\": return nn.GELU()\n",
        "    raise ValueError(f'{activation} is not available. You can use \"relu\", \"gelu\", or a callable')\n",
        "\n",
        "\n",
        "# decomposition\n",
        "\n",
        "class moving_avg(nn.Module):\n",
        "    \"\"\"\n",
        "    Moving average block to highlight the trend of time series\n",
        "    \"\"\"\n",
        "    def __init__(self, kernel_size, stride):\n",
        "        super(moving_avg, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # padding on the both ends of time series\n",
        "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        x = torch.cat([front, x, end], dim=1)\n",
        "        x = self.avg(x.permute(0, 2, 1))\n",
        "        x = x.permute(0, 2, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class series_decomp(nn.Module):\n",
        "    \"\"\"\n",
        "    Series decomposition block\n",
        "    \"\"\"\n",
        "    def __init__(self, kernel_size):\n",
        "        super(series_decomp, self).__init__()\n",
        "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        moving_mean = self.moving_avg(x)\n",
        "        res = x - moving_mean\n",
        "        return res, moving_mean\n",
        "\n",
        "\n",
        "\n",
        "# pos_encoding\n",
        "\n",
        "def PositionalEncoding(q_len, d_model, normalize=True):\n",
        "    pe = torch.zeros(q_len, d_model)\n",
        "    position = torch.arange(0, q_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    if normalize:\n",
        "        pe = pe - pe.mean()\n",
        "        pe = pe / (pe.std() * 10)\n",
        "    return pe\n",
        "\n",
        "SinCosPosEncoding = PositionalEncoding\n",
        "\n",
        "def Coord2dPosEncoding(q_len, d_model, exponential=False, normalize=True, eps=1e-3, verbose=False):\n",
        "    x = .5 if exponential else 1\n",
        "    i = 0\n",
        "    for i in range(100):\n",
        "        cpe = 2 * (torch.linspace(0, 1, q_len).reshape(-1, 1) ** x) * (torch.linspace(0, 1, d_model).reshape(1, -1) ** x) - 1\n",
        "        pv(f'{i:4.0f}  {x:5.3f}  {cpe.mean():+6.3f}', verbose)\n",
        "        if abs(cpe.mean()) <= eps: break\n",
        "        elif cpe.mean() > eps: x += .001\n",
        "        else: x -= .001\n",
        "        i += 1\n",
        "    if normalize:\n",
        "        cpe = cpe - cpe.mean()\n",
        "        cpe = cpe / (cpe.std() * 10)\n",
        "    return cpe\n",
        "\n",
        "def Coord1dPosEncoding(q_len, exponential=False, normalize=True):\n",
        "    cpe = (2 * (torch.linspace(0, 1, q_len).reshape(-1, 1)**(.5 if exponential else 1)) - 1)\n",
        "    if normalize:\n",
        "        cpe = cpe - cpe.mean()\n",
        "        cpe = cpe / (cpe.std() * 10)\n",
        "    return cpe\n",
        "\n",
        "def positional_encoding(pe, learn_pe, q_len, d_model):\n",
        "    # Positional encoding\n",
        "    if pe == None:\n",
        "        W_pos = torch.empty((q_len, d_model)) # pe = None and learn_pe = False can be used to measure impact of pe\n",
        "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
        "        learn_pe = False\n",
        "    elif pe == 'zero':\n",
        "        W_pos = torch.empty((q_len, 1))\n",
        "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
        "    elif pe == 'zeros':\n",
        "        W_pos = torch.empty((q_len, d_model))\n",
        "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
        "    elif pe == 'normal' or pe == 'gauss':\n",
        "        W_pos = torch.zeros((q_len, 1))\n",
        "        torch.nn.init.normal_(W_pos, mean=0.0, std=0.1)\n",
        "    elif pe == 'uniform':\n",
        "        W_pos = torch.zeros((q_len, 1))\n",
        "        nn.init.uniform_(W_pos, a=0.0, b=0.1)\n",
        "    elif pe == 'lin1d': W_pos = Coord1dPosEncoding(q_len, exponential=False, normalize=True)\n",
        "    elif pe == 'exp1d': W_pos = Coord1dPosEncoding(q_len, exponential=True, normalize=True)\n",
        "    elif pe == 'lin2d': W_pos = Coord2dPosEncoding(q_len, d_model, exponential=False, normalize=True)\n",
        "    elif pe == 'exp2d': W_pos = Coord2dPosEncoding(q_len, d_model, exponential=True, normalize=True)\n",
        "    elif pe == 'sincos': W_pos = PositionalEncoding(q_len, d_model, normalize=True)\n",
        "    else: raise ValueError(f\"{pe} is not a valid pe (positional encoder. Available types: 'gauss'=='normal', \\\n",
        "        'zeros', 'zero', uniform', 'lin1d', 'exp1d', 'lin2d', 'exp2d', 'sincos', None.)\")\n",
        "    return nn.Parameter(W_pos, requires_grad=learn_pe)"
      ],
      "metadata": {
        "id": "qfc-fuNPspZA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PatchTST"
      ],
      "metadata": {
        "id": "icJ_LAmxuUPQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "jM03X-gXp8Cb"
      },
      "outputs": [],
      "source": [
        "class PatchTST(nn.Module):\n",
        "    def __init__(self,\n",
        "                 configs,\n",
        "                 max_seq_len:Optional[int]=1024,\n",
        "                 d_k:Optional[int]=None,\n",
        "                 d_v:Optional[int]=None,\n",
        "                 norm:str='BatchNorm',\n",
        "                 attn_dropout:float=0.,\n",
        "                 act:str=\"gelu\",\n",
        "                 key_padding_mask:bool='auto',\n",
        "                 padding_var:Optional[int]=None,\n",
        "                 attn_mask:Optional[Tensor]=None,\n",
        "                 res_attention:bool=True,\n",
        "                 pre_norm:bool=False,\n",
        "                 store_attn:bool=False,\n",
        "                 pe:str='zeros',\n",
        "                 learn_pe:bool=True,\n",
        "                 pretrain_head:bool=False,\n",
        "                 head_type = 'flatten',\n",
        "                 verbose:bool=False,\n",
        "                 **kwargs):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # load parameters\n",
        "        c_in = configs.enc_in\n",
        "        context_window = configs.seq_len\n",
        "        target_window = configs.pred_len\n",
        "\n",
        "        n_layers = configs.e_layers\n",
        "        n_heads = configs.n_heads\n",
        "        d_model = configs.d_model\n",
        "        d_ff = configs.d_ff\n",
        "        dropout = configs.dropout\n",
        "        fc_dropout = configs.fc_dropout\n",
        "        head_dropout = configs.head_dropout\n",
        "\n",
        "        individual = configs.individual\n",
        "\n",
        "        patch_len = configs.patch_len\n",
        "        stride = configs.stride\n",
        "        padding_patch = configs.padding_patch\n",
        "\n",
        "        revin = configs.revin\n",
        "        affine = configs.affine\n",
        "        subtract_last = configs.subtract_last\n",
        "\n",
        "        decomposition = configs.decomposition\n",
        "        kernel_size = configs.kernel_size\n",
        "\n",
        "\n",
        "        # model\n",
        "        self.decomposition = decomposition\n",
        "        if self.decomposition:\n",
        "            self.decomp_module = series_decomp(kernel_size)\n",
        "            self.model_trend = PatchTST_backbone(c_in=c_in, context_window = context_window, target_window=target_window, patch_len=patch_len, stride=stride,\n",
        "                                  max_seq_len=max_seq_len, n_layers=n_layers, d_model=d_model,\n",
        "                                  n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout,\n",
        "                                  dropout=dropout, act=act, key_padding_mask=key_padding_mask, padding_var=padding_var,\n",
        "                                  attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n",
        "                                  pe=pe, learn_pe=learn_pe, fc_dropout=fc_dropout, head_dropout=head_dropout, padding_patch = padding_patch,\n",
        "                                  pretrain_head=pretrain_head, head_type=head_type, individual=individual, revin=revin, affine=affine,\n",
        "                                  subtract_last=subtract_last, verbose=verbose, **kwargs)\n",
        "            self.model_res = PatchTST_backbone(c_in=c_in, context_window = context_window, target_window=target_window, patch_len=patch_len, stride=stride,\n",
        "                                  max_seq_len=max_seq_len, n_layers=n_layers, d_model=d_model,\n",
        "                                  n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout,\n",
        "                                  dropout=dropout, act=act, key_padding_mask=key_padding_mask, padding_var=padding_var,\n",
        "                                  attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n",
        "                                  pe=pe, learn_pe=learn_pe, fc_dropout=fc_dropout, head_dropout=head_dropout, padding_patch = padding_patch,\n",
        "                                  pretrain_head=pretrain_head, head_type=head_type, individual=individual, revin=revin, affine=affine,\n",
        "                                  subtract_last=subtract_last, verbose=verbose, **kwargs)\n",
        "        else:\n",
        "            self.model = PatchTST_backbone(c_in=c_in, context_window = context_window, target_window=target_window, patch_len=patch_len, stride=stride,\n",
        "                                  max_seq_len=max_seq_len, n_layers=n_layers, d_model=d_model,\n",
        "                                  n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout,\n",
        "                                  dropout=dropout, act=act, key_padding_mask=key_padding_mask, padding_var=padding_var,\n",
        "                                  attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n",
        "                                  pe=pe, learn_pe=learn_pe, fc_dropout=fc_dropout, head_dropout=head_dropout, padding_patch = padding_patch,\n",
        "                                  pretrain_head=pretrain_head, head_type=head_type, individual=individual, revin=revin, affine=affine,\n",
        "                                  subtract_last=subtract_last, verbose=verbose, **kwargs)\n",
        "\n",
        "\n",
        "    def forward(self, x):           # x: [Batch, Input length, Channel]\n",
        "        if self.decomposition:\n",
        "            res_init, trend_init = self.decomp_module(x)\n",
        "            res_init, trend_init = res_init.permute(0,2,1), trend_init.permute(0,2,1)  # x: [Batch, Channel, Input length]\n",
        "            res = self.model_res(res_init)\n",
        "            trend = self.model_trend(trend_init)\n",
        "            x = res + trend\n",
        "            x = x.permute(0,2,1)    # x: [Batch, Input length, Channel]\n",
        "        else:\n",
        "            x = x.permute(0,2,1)    # x: [Batch, Channel, Input length]\n",
        "            x = self.model(x)       # x: [Batch, Channel, Input length] -> PatchTST_backbone()\n",
        "            x = x.permute(0,2,1)    # x: [Batch, Output length, Channel]\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# Cell\n",
        "class PatchTST_backbone(nn.Module):\n",
        "    def __init__(self,\n",
        "                 c_in:int,\n",
        "                 context_window:int,\n",
        "                 target_window:int,\n",
        "                 patch_len:int,\n",
        "                 stride:int,\n",
        "                 max_seq_len:Optional[int]=1024,\n",
        "                 n_layers:int=3,\n",
        "                 d_model=128,\n",
        "                 n_heads=16,\n",
        "                 d_k:Optional[int]=None,\n",
        "                 d_v:Optional[int]=None,\n",
        "                 d_ff:int=256,\n",
        "                 norm:str='BatchNorm',\n",
        "                 attn_dropout:float=0.,\n",
        "                 dropout:float=0.,\n",
        "                 act:str=\"gelu\",\n",
        "                 key_padding_mask:bool='auto',\n",
        "                 padding_var:Optional[int]=None,\n",
        "                 attn_mask:Optional[Tensor]=None,\n",
        "                 res_attention:bool=True,\n",
        "                 pre_norm:bool=False,\n",
        "                 store_attn:bool=False,\n",
        "                 pe:str='zeros',\n",
        "                 learn_pe:bool=True,\n",
        "                 fc_dropout:float=0.,\n",
        "                 head_dropout = 0,\n",
        "                 padding_patch = None,\n",
        "                 pretrain_head:bool=False,\n",
        "                 head_type = 'flatten',\n",
        "                 individual = False,\n",
        "                 revin = True,\n",
        "                 affine = True,\n",
        "                 subtract_last = False,\n",
        "                 verbose:bool=False,\n",
        "                 **kwargs):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # RevIn\n",
        "        self.revin = revin\n",
        "        if self.revin: self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last)\n",
        "\n",
        "        # Patching\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.padding_patch = padding_patch\n",
        "        patch_num = int((context_window - patch_len)/stride + 1)\n",
        "        if padding_patch == 'end': # can be modified to general case\n",
        "            self.padding_patch_layer = nn.ReplicationPad1d((0, stride))\n",
        "            patch_num += 1\n",
        "\n",
        "        # Backbone\n",
        "        self.backbone = TSTiEncoder(c_in, patch_num=patch_num, patch_len=patch_len, max_seq_len=max_seq_len,\n",
        "                                n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,\n",
        "                                attn_dropout=attn_dropout, dropout=dropout, act=act, key_padding_mask=key_padding_mask, padding_var=padding_var,\n",
        "                                attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n",
        "                                pe=pe, learn_pe=learn_pe, verbose=verbose, **kwargs)\n",
        "\n",
        "        # Head\n",
        "        self.head_nf = d_model * patch_num\n",
        "        self.n_vars = c_in\n",
        "        self.pretrain_head = pretrain_head\n",
        "        self.head_type = head_type\n",
        "        self.individual = individual\n",
        "\n",
        "        if self.pretrain_head:\n",
        "            self.head = self.create_pretrain_head(self.head_nf, c_in, fc_dropout) # custom head passed as a partial func with all its kwargs\n",
        "        elif head_type == 'flatten':\n",
        "            self.head = Flatten_Head(self.individual, self.n_vars, self.head_nf, target_window, head_dropout=head_dropout)\n",
        "\n",
        "\n",
        "    def forward(self, z):                                                                   # z: [bs, c, seq_len]\n",
        "        # norm\n",
        "        if self.revin:\n",
        "            z = z.permute(0,2,1)                                                            # z: [bs, seq_len, c]\n",
        "            z = self.revin_layer(z, 'norm')                                                 # z: [bs, seq_len, c]\n",
        "            z = z.permute(0,2,1)                                                            # z: [bs, c, seq_len]\n",
        "\n",
        "        # do patching\n",
        "        if self.padding_patch == 'end':\n",
        "            z = self.padding_patch_layer(z)                                                 # z: [bs, c, seq_len+pad]  -> do padding on the \"end\" of seq_len or the last one\n",
        "        z = z.unfold(dimension=-1, size=self.patch_len, step=self.stride)                   # z: [bs x nvars x patch_num x patch_len]\n",
        "        z = z.permute(0,1,3,2)                                                              # z: [bs x nvars x patch_len x patch_num]\n",
        "\n",
        "        # model\n",
        "        z = self.backbone(z)                                                                # z: [bs x nvars x d_model x patch_num]\n",
        "        z = self.head(z)                                                                    # z: [bs x nvars x target_window]\n",
        "\n",
        "        # denorm\n",
        "        if self.revin:\n",
        "            z = z.permute(0,2,1)\n",
        "            z = self.revin_layer(z, 'denorm')\n",
        "            z = z.permute(0,2,1)\n",
        "        return z\n",
        "\n",
        "    def create_pretrain_head(self, head_nf, vars, dropout):\n",
        "        return nn.Sequential(nn.Dropout(dropout),\n",
        "                    nn.Conv1d(head_nf, vars, 1)\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = PatchTST(args)\n",
        "summary(model=model,\n",
        "        input_size=([args.batch_size, args.seq_len, args.enc_in]),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "gpbINQH044qz",
        "outputId": "8d6d6d3f-ba05-4a64-f36b-5527c3d6226a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===========================================================================================================================================================\n",
              "Layer (type (var_name))                                                     Input Shape          Output Shape         Param #              Trainable\n",
              "===========================================================================================================================================================\n",
              "PatchTST (PatchTST)                                                         [128, 336, 21]       [128, 96, 21]        --                   Partial\n",
              "├─PatchTST_backbone (model)                                                 [128, 21, 336]       [128, 21, 96]        --                   Partial\n",
              "│    └─RevIN (revin_layer)                                                  [128, 336, 21]       [128, 336, 21]       --                   --\n",
              "│    └─ReplicationPad1d (padding_patch_layer)                               [128, 21, 336]       [128, 21, 344]       --                   --\n",
              "│    └─TSTiEncoder (backbone)                                               [128, 21, 16, 42]    [128, 21, 128, 42]   5,376                Partial\n",
              "│    │    └─Linear (W_P)                                                    [128, 21, 42, 16]    [128, 21, 42, 128]   2,176                True\n",
              "│    │    └─Dropout (dropout)                                               [2688, 42, 128]      [2688, 42, 128]      --                   --\n",
              "│    │    └─TSTEncoder (encoder)                                            [2688, 42, 128]      [2688, 42, 128]      397,443              Partial\n",
              "│    └─Flatten_Head (head)                                                  [128, 21, 128, 42]   [128, 21, 96]        --                   True\n",
              "│    │    └─Flatten (flatten)                                               [128, 21, 128, 42]   [128, 21, 5376]      --                   --\n",
              "│    │    └─Linear (linear)                                                 [128, 21, 5376]      [128, 21, 96]        516,192              True\n",
              "│    │    └─Dropout (dropout)                                               [128, 21, 96]        [128, 21, 96]        --                   --\n",
              "│    └─RevIN (revin_layer)                                                  [128, 96, 21]        [128, 96, 21]        --                   --\n",
              "===========================================================================================================================================================\n",
              "Total params: 921,187\n",
              "Trainable params: 921,184\n",
              "Non-trainable params: 3\n",
              "Total mult-adds (G): 1.13\n",
              "===========================================================================================================================================================\n",
              "Input size (MB): 3.61\n",
              "Forward/backward pass size (MB): 3239.02\n",
              "Params size (MB): 3.66\n",
              "Estimated Total Size (MB): 3246.29\n",
              "==========================================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReVIN"
      ],
      "metadata": {
        "id": "nPR0RH2EuKcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RevIN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_features: int,\n",
        "                 eps=1e-5,\n",
        "                 affine=True,\n",
        "                 subtract_last=False\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        :param num_features: the number of features or channels\n",
        "        :param eps: a value added for numerical stability\n",
        "        :param affine: if True, RevIN has learnable affine parameters\n",
        "        \"\"\"\n",
        "        super(RevIN, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "        self.subtract_last = subtract_last\n",
        "        if self.affine:\n",
        "            self._init_params()\n",
        "\n",
        "    def forward(self, x, mode:str):\n",
        "        if mode == 'norm':\n",
        "            self._get_statistics(x)\n",
        "            x = self._normalize(x)\n",
        "        elif mode == 'denorm':\n",
        "            x = self._denormalize(x)\n",
        "        else: raise NotImplementedError\n",
        "        return x\n",
        "\n",
        "    def _init_params(self):\n",
        "        # initialize RevIN params: (C,)\n",
        "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
        "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
        "\n",
        "    def _get_statistics(self, x):\n",
        "        dim2reduce = tuple(range(1, x.ndim-1))\n",
        "        if self.subtract_last:\n",
        "            self.last = x[:,-1,:].unsqueeze(1)\n",
        "        else:\n",
        "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
        "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
        "\n",
        "    def _normalize(self, x):\n",
        "        if self.subtract_last:\n",
        "            x = x - self.last\n",
        "        else:\n",
        "            x = x - self.mean\n",
        "        x = x / self.stdev\n",
        "        if self.affine:\n",
        "            x = x * self.affine_weight\n",
        "            x = x + self.affine_bias\n",
        "        return x\n",
        "\n",
        "    def _denormalize(self, x):\n",
        "        if self.affine:\n",
        "            x = x - self.affine_bias\n",
        "            x = x / (self.affine_weight + self.eps*self.eps)\n",
        "        x = x * self.stdev\n",
        "        if self.subtract_last:\n",
        "            x = x + self.last\n",
        "        else:\n",
        "            x = x + self.mean\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "aHNYxeo0rvCZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TSTEncoder"
      ],
      "metadata": {
        "id": "oxwSWtFFuadY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TSTiEncoder(nn.Module):  #i means channel-independent\n",
        "    def __init__(self, c_in, patch_num, patch_len, max_seq_len=1024,\n",
        "                 n_layers=3, d_model=128, n_heads=16, d_k=None, d_v=None,\n",
        "                 d_ff=256, norm='BatchNorm', attn_dropout=0., dropout=0., act=\"gelu\", store_attn=False,\n",
        "                 key_padding_mask='auto', padding_var=None, attn_mask=None, res_attention=True, pre_norm=False,\n",
        "                 pe='zeros', learn_pe=True, verbose=False, **kwargs):\n",
        "\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_num = patch_num\n",
        "        self.patch_len = patch_len\n",
        "\n",
        "        # Input encoding\n",
        "        q_len = patch_num\n",
        "        self.W_P = nn.Linear(patch_len, d_model)        # Eq 1: projection of feature vectors onto a d-dim vector space\n",
        "        self.seq_len = q_len\n",
        "\n",
        "        # Positional encoding\n",
        "        self.W_pos = positional_encoding(pe, learn_pe, q_len, d_model)\n",
        "\n",
        "        # Residual dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = TSTEncoder(q_len, d_model, n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout, dropout=dropout,\n",
        "                                   pre_norm=pre_norm, activation=act, res_attention=res_attention, n_layers=n_layers, store_attn=store_attn)\n",
        "\n",
        "\n",
        "    def forward(self, x) -> Tensor:                                              # x: [bs, c, patch_len, patch_num]\n",
        "\n",
        "        n_vars = x.shape[1]\n",
        "        # Input encoding\n",
        "        x = x.permute(0,1,3,2)                                                   # x: [bs, c, patch_num, patch_len]\n",
        "        x = self.W_P(x)                                                          # x: [bs, c, patch_num, d_model]\n",
        "\n",
        "        u = torch.reshape(x, (x.shape[0]*x.shape[1],x.shape[2],x.shape[3]))      # u: [bs * c x patch_num x d_model]\n",
        "        u = self.dropout(u + self.W_pos)                                         # u: [bs * c x patch_num x d_model]\n",
        "\n",
        "        # Encoder\n",
        "        z = self.encoder(u)                                                      # z: [bs * c x patch_num x d_model]\n",
        "        z = torch.reshape(z, (-1,n_vars,z.shape[-2],z.shape[-1]))                # z: [bs x c x patch_num x d_model]\n",
        "        z = z.permute(0,1,3,2)                                                   # z: [bs x c x d_model x patch_num]\n",
        "\n",
        "        return z"
      ],
      "metadata": {
        "id": "lKYfGMRD3rVv"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TSTEncoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 q_len,\n",
        "                 d_model,\n",
        "                 n_heads,\n",
        "                 d_k=None,\n",
        "                 d_v=None,\n",
        "                 d_ff=256,\n",
        "                 store_attn=False,\n",
        "                 norm='BatchNorm',\n",
        "                 attn_dropout=0,\n",
        "                 dropout=0.,\n",
        "                 bias=True,\n",
        "                 activation=\"gelu\",\n",
        "                 res_attention=False,\n",
        "                 pre_norm=False\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        assert not d_model%n_heads, f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n",
        "        d_k = d_model // n_heads if d_k is None else d_k\n",
        "        d_v = d_model // n_heads if d_v is None else d_v\n",
        "\n",
        "        # Multi-Head attention\n",
        "        self.res_attention = res_attention\n",
        "        self.self_attn = _MultiheadAttention(d_model, n_heads, d_k, d_v, attn_dropout=attn_dropout, proj_dropout=dropout, res_attention=res_attention)\n",
        "\n",
        "        # Add & Norm\n",
        "        self.dropout_attn = nn.Dropout(dropout)\n",
        "        if \"batch\" in norm.lower():\n",
        "            self.norm_attn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
        "        else:\n",
        "            self.norm_attn = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Position-wise Feed-Forward\n",
        "        self.ff = nn.Sequential(nn.Linear(d_model, d_ff, bias=bias),\n",
        "                                get_activation_fn(activation),\n",
        "                                nn.Dropout(dropout),\n",
        "                                nn.Linear(d_ff, d_model, bias=bias))\n",
        "\n",
        "        # Add & Norm\n",
        "        self.dropout_ffn = nn.Dropout(dropout)\n",
        "        if \"batch\" in norm.lower():\n",
        "            self.norm_ffn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
        "        else:\n",
        "            self.norm_ffn = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.pre_norm = pre_norm\n",
        "        self.store_attn = store_attn\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                src:Tensor,\n",
        "                prev:Optional[Tensor]=None,\n",
        "                key_padding_mask:Optional[Tensor]=None,\n",
        "                attn_mask:Optional[Tensor]=None) -> Tensor:\n",
        "\n",
        "        # Multi-Head attention sublayer\n",
        "        if self.pre_norm:\n",
        "            src = self.norm_attn(src)\n",
        "        ## Multi-Head attention\n",
        "        if self.res_attention:\n",
        "            src2, attn, scores = self.self_attn(src, src, src, prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        else:\n",
        "            src2, attn = self.self_attn(src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        if self.store_attn:\n",
        "            self.attn = attn\n",
        "        ## Add & Norm\n",
        "        src = src + self.dropout_attn(src2) # Add: residual connection with residual dropout\n",
        "        if not self.pre_norm:\n",
        "            src = self.norm_attn(src)\n",
        "\n",
        "        # Feed-forward sublayer\n",
        "        if self.pre_norm:\n",
        "            src = self.norm_ffn(src)\n",
        "        ## Position-wise Feed-Forward\n",
        "        src2 = self.ff(src)\n",
        "        ## Add & Norm\n",
        "        src = src + self.dropout_ffn(src2) # Add: residual connection with residual dropout\n",
        "        if not self.pre_norm:\n",
        "            src = self.norm_ffn(src)\n",
        "\n",
        "        if self.res_attention:\n",
        "            return src, scores\n",
        "        else:\n",
        "            return src\n",
        "\n",
        "# Cell\n",
        "class TSTEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 q_len,\n",
        "                 d_model,\n",
        "                 n_heads,\n",
        "                 d_k=None,\n",
        "                 d_v=None,\n",
        "                 d_ff=None,\n",
        "                 norm='BatchNorm',\n",
        "                 attn_dropout=0.,\n",
        "                 dropout=0.,\n",
        "                 activation='gelu',\n",
        "                 res_attention=False,\n",
        "                 n_layers=1,\n",
        "                 pre_norm=False,\n",
        "                 store_attn=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList([TSTEncoderLayer(q_len, d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm,\n",
        "                                                      attn_dropout=attn_dropout, dropout=dropout,\n",
        "                                                      activation=activation, res_attention=res_attention,\n",
        "                                                      pre_norm=pre_norm, store_attn=store_attn) for i in range(n_layers)])\n",
        "        self.res_attention = res_attention\n",
        "\n",
        "    def forward(self,\n",
        "                src:Tensor,\n",
        "                key_padding_mask:Optional[Tensor]=None,\n",
        "                attn_mask:Optional[Tensor]=None):\n",
        "        output = src\n",
        "        scores = None\n",
        "        if self.res_attention:\n",
        "            for mod in self.layers: output, scores = mod(output, prev=scores, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "            return output\n",
        "        else:\n",
        "            for mod in self.layers: output = mod(output, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "            return output\n"
      ],
      "metadata": {
        "id": "FK606phWrVz2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flatten_Head"
      ],
      "metadata": {
        "id": "-Rwj3xkmuYnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten_Head(nn.Module):\n",
        "    def __init__(self,\n",
        "                 individual,\n",
        "                 n_vars, nf,\n",
        "                 target_window,\n",
        "                 head_dropout=0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.individual = individual\n",
        "        self.n_vars = n_vars\n",
        "\n",
        "        if self.individual:\n",
        "            self.linears = nn.ModuleList()\n",
        "            self.dropouts = nn.ModuleList()\n",
        "            self.flattens = nn.ModuleList()\n",
        "            for i in range(self.n_vars):\n",
        "                self.flattens.append(nn.Flatten(start_dim=-2))\n",
        "                self.linears.append(nn.Linear(nf, target_window))\n",
        "                self.dropouts.append(nn.Dropout(head_dropout))\n",
        "        else:\n",
        "            self.flatten = nn.Flatten(start_dim=-2)\n",
        "            self.linear = nn.Linear(nf, target_window)\n",
        "            self.dropout = nn.Dropout(head_dropout)\n",
        "\n",
        "    def forward(self, x):                                 # x: [bs x nvars x d_model x patch_num]\n",
        "        if self.individual:\n",
        "            x_out = []\n",
        "            for i in range(self.n_vars):\n",
        "                z = self.flattens[i](x[:,i,:,:])          # z: [bs x d_model * patch_num]\n",
        "                z = self.linears[i](z)                    # z: [bs x target_window]\n",
        "                z = self.dropouts[i](z)\n",
        "                x_out.append(z)\n",
        "            x = torch.stack(x_out, dim=1)                 # x: [bs x nvars x target_window]\n",
        "        else:\n",
        "            x = self.flatten(x)\n",
        "            x = self.linear(x)\n",
        "            x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "dXleXyxlrMha"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MultiheadAttention"
      ],
      "metadata": {
        "id": "26NWzljouhSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class _MultiheadAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 n_heads,\n",
        "                 d_k=None,\n",
        "                 d_v=None,\n",
        "                 res_attention=False,\n",
        "                 attn_dropout=0.,\n",
        "                 proj_dropout=0.,\n",
        "                 qkv_bias=True,\n",
        "                 lsa=False):\n",
        "        \"\"\"Multi Head Attention Layer\n",
        "        Input shape:\n",
        "            Q:       [batch_size (bs) x max_q_len x d_model]\n",
        "            K, V:    [batch_size (bs) x q_len x d_model]\n",
        "            mask:    [q_len x q_len]\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        d_k = d_model // n_heads if d_k is None else d_k\n",
        "        d_v = d_model // n_heads if d_v is None else d_v\n",
        "\n",
        "        self.n_heads, self.d_k, self.d_v = n_heads, d_k, d_v\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=qkv_bias)\n",
        "\n",
        "        # Scaled Dot-Product Attention (multiple heads)\n",
        "        self.res_attention = res_attention\n",
        "        self.sdp_attn = _ScaledDotProductAttention(d_model, n_heads, attn_dropout=attn_dropout, res_attention=self.res_attention, lsa=lsa)\n",
        "\n",
        "        # Poject output\n",
        "        self.to_out = nn.Sequential(nn.Linear(n_heads * d_v, d_model), nn.Dropout(proj_dropout))\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                Q:Tensor,\n",
        "                K:Optional[Tensor]=None,\n",
        "                V:Optional[Tensor]=None,\n",
        "                prev:Optional[Tensor]=None,\n",
        "                key_padding_mask:Optional[Tensor]=None,\n",
        "                attn_mask:Optional[Tensor]=None):\n",
        "\n",
        "        bs = Q.size(0)\n",
        "        if K is None: K = Q\n",
        "        if V is None: V = Q\n",
        "\n",
        "        # Linear (+ split in multiple heads)\n",
        "        q_s = self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1,2)       # q_s    : [bs x n_heads x max_q_len x d_k]\n",
        "        k_s = self.W_K(K).view(bs, -1, self.n_heads, self.d_k).permute(0,2,3,1)     # k_s    : [bs x n_heads x d_k x q_len] - transpose(1,2) + transpose(2,3)\n",
        "        v_s = self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1,2)       # v_s    : [bs x n_heads x q_len x d_v]\n",
        "\n",
        "        # Apply Scaled Dot-Product Attention (multiple heads)\n",
        "        if self.res_attention:\n",
        "            output, attn_weights, attn_scores = self.sdp_attn(q_s, k_s, v_s, prev=prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        else:\n",
        "            output, attn_weights = self.sdp_attn(q_s, k_s, v_s, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        # output: [bs x n_heads x q_len x d_v], attn: [bs x n_heads x q_len x q_len], scores: [bs x n_heads x max_q_len x q_len]\n",
        "\n",
        "        # back to the original inputs dimensions\n",
        "        output = output.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v) # output: [bs x q_len x n_heads * d_v]\n",
        "        output = self.to_out(output)\n",
        "\n",
        "        if self.res_attention: return output, attn_weights, attn_scores\n",
        "        else: return output, attn_weights\n",
        "\n",
        "\n",
        "class _ScaledDotProductAttention(nn.Module):\n",
        "    r\"\"\"Scaled Dot-Product Attention module (Attention is all you need by Vaswani et al., 2017) with optional residual attention from previous layer\n",
        "    (Realformer: Transformer likes residual attention by He et al, 2020) and locality self sttention (Vision Transformer for Small-Size Datasets\n",
        "    by Lee et al, 2021)\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 n_heads,\n",
        "                 attn_dropout=0.,\n",
        "                 res_attention=False,\n",
        "                 lsa=False):\n",
        "        super().__init__()\n",
        "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "        self.res_attention = res_attention\n",
        "        head_dim = d_model // n_heads\n",
        "        self.scale = nn.Parameter(torch.tensor(head_dim ** -0.5), requires_grad=lsa)\n",
        "        self.lsa = lsa\n",
        "\n",
        "    def forward(self,\n",
        "                q:Tensor,\n",
        "                k:Tensor,\n",
        "                v:Tensor,\n",
        "                prev:Optional[Tensor]=None,\n",
        "                key_padding_mask:Optional[Tensor]=None,\n",
        "                attn_mask:Optional[Tensor]=None):\n",
        "        '''\n",
        "        Input shape:\n",
        "            q               : [bs x n_heads x max_q_len x d_k]\n",
        "            k               : [bs x n_heads x d_k x seq_len]\n",
        "            v               : [bs x n_heads x seq_len x d_v]\n",
        "            prev            : [bs x n_heads x q_len x seq_len]\n",
        "            key_padding_mask: [bs x seq_len]\n",
        "            attn_mask       : [1 x seq_len x seq_len]\n",
        "        Output shape:\n",
        "            output:  [bs x n_heads x q_len x d_v]\n",
        "            attn   : [bs x n_heads x q_len x seq_len]\n",
        "            scores : [bs x n_heads x q_len x seq_len]\n",
        "        '''\n",
        "\n",
        "        # Scaled MatMul (q, k) - similarity scores for all pairs of positions in an input sequence\n",
        "        attn_scores = torch.matmul(q, k) * self.scale      # attn_scores : [bs x n_heads x max_q_len x q_len]\n",
        "\n",
        "        # Add pre-softmax attention scores from the previous layer (optional)\n",
        "        if prev is not None: attn_scores = attn_scores + prev\n",
        "\n",
        "        # Attention mask (optional)\n",
        "        if attn_mask is not None:                                     # attn_mask with shape [q_len x seq_len] - only used when q_len == seq_len\n",
        "            if attn_mask.dtype == torch.bool:\n",
        "                attn_scores.masked_fill_(attn_mask, -np.inf)\n",
        "            else:\n",
        "                attn_scores += attn_mask\n",
        "\n",
        "        # Key padding mask (optional)\n",
        "        if key_padding_mask is not None:                              # mask with shape [bs x q_len] (only when max_w_len == q_len)\n",
        "            attn_scores.masked_fill_(key_padding_mask.unsqueeze(1).unsqueeze(2), -np.inf)\n",
        "\n",
        "        # normalize the attention weights\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)                 # attn_weights   : [bs x n_heads x max_q_len x q_len]\n",
        "        attn_weights = self.attn_dropout(attn_weights)\n",
        "\n",
        "        # compute the new values given the attention weights\n",
        "        output = torch.matmul(attn_weights, v)                        # output: [bs x n_heads x max_q_len x d_v]\n",
        "\n",
        "        if self.res_attention: return output, attn_weights, attn_scores\n",
        "        else: return output, attn_weights"
      ],
      "metadata": {
        "id": "KHpq1mxHrhVD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1alE33S1GmP5wACMXaLu50rDIoVzBM4ik"
      ],
      "metadata": {
        "id": "1qyU3Zmmr3hL",
        "outputId": "0d5e1842-f13b-4e1d-aad6-b301e0bf83f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1alE33S1GmP5wACMXaLu50rDIoVzBM4ik\n",
            "To: /content/all_six_datasets.zip\n",
            "100% 54.0M/54.0M [00:02<00:00, 25.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq all_six_datasets.zip"
      ],
      "metadata": {
        "id": "ZytZ6JVSwqqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ogrKYiCbxdcG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/all_six_datasets/weather/weather.csv\")\n",
        "data = data.set_index(\"date\")\n",
        "data.index = pd.to_datetime(data.index)\n",
        "data"
      ],
      "metadata": {
        "id": "vzBfEisJxnWt",
        "outputId": "8cc26037-7019-4ee1-e83d-a298c3e1102b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     p (mbar)  T (degC)  Tpot (K)  Tdew (degC)  rh (%)  \\\n",
              "date                                                                     \n",
              "2020-01-01 00:10:00   1008.89      0.71    273.18        -1.33    86.1   \n",
              "2020-01-01 00:20:00   1008.76      0.75    273.22        -1.44    85.2   \n",
              "2020-01-01 00:30:00   1008.66      0.73    273.21        -1.48    85.1   \n",
              "2020-01-01 00:40:00   1008.64      0.37    272.86        -1.64    86.3   \n",
              "2020-01-01 00:50:00   1008.61      0.33    272.82        -1.50    87.4   \n",
              "...                       ...       ...       ...          ...     ...   \n",
              "2020-12-31 23:20:00    978.32      2.28    277.16        -0.80    80.0   \n",
              "2020-12-31 23:30:00    978.30      2.13    277.01        -0.43    83.1   \n",
              "2020-12-31 23:40:00    978.26      1.99    276.88        -0.71    82.2   \n",
              "2020-12-31 23:50:00    978.26      2.07    276.95        -0.77    81.4   \n",
              "2021-01-01 00:00:00    978.24      2.01    276.89        -0.66    82.4   \n",
              "\n",
              "                     VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  \\\n",
              "date                                                                       \n",
              "2020-01-01 00:10:00          6.43          5.54          0.89       3.42   \n",
              "2020-01-01 00:20:00          6.45          5.49          0.95       3.39   \n",
              "2020-01-01 00:30:00          6.44          5.48          0.96       3.39   \n",
              "2020-01-01 00:40:00          6.27          5.41          0.86       3.35   \n",
              "2020-01-01 00:50:00          6.26          5.47          0.79       3.38   \n",
              "...                           ...           ...           ...        ...   \n",
              "2020-12-31 23:20:00          7.20          5.76          1.44       3.67   \n",
              "2020-12-31 23:30:00          7.12          5.92          1.20       3.77   \n",
              "2020-12-31 23:40:00          7.05          5.80          1.26       3.69   \n",
              "2020-12-31 23:50:00          7.09          5.77          1.32       3.68   \n",
              "2021-01-01 00:00:00          7.06          5.82          1.24       3.71   \n",
              "\n",
              "                     H2OC (mmol/mol)  ...  wv (m/s)  max. wv (m/s)  wd (deg)  \\\n",
              "date                                  ...                                      \n",
              "2020-01-01 00:10:00             5.49  ...      1.02           1.60     224.3   \n",
              "2020-01-01 00:20:00             5.45  ...      0.43           0.84     206.8   \n",
              "2020-01-01 00:30:00             5.43  ...      0.61           1.48     197.1   \n",
              "2020-01-01 00:40:00             5.37  ...      1.11           1.48     206.4   \n",
              "2020-01-01 00:50:00             5.42  ...      0.49           1.40     209.6   \n",
              "...                              ...  ...       ...            ...       ...   \n",
              "2020-12-31 23:20:00             5.89  ...      0.73           1.40     180.6   \n",
              "2020-12-31 23:30:00             6.05  ...      0.43           0.82     174.0   \n",
              "2020-12-31 23:40:00             5.93  ...      0.38           0.76     248.9   \n",
              "2020-12-31 23:50:00             5.90  ...      0.57           1.07     196.6   \n",
              "2021-01-01 00:00:00             5.95  ...      0.57           1.08     221.3   \n",
              "\n",
              "                     rain (mm)  raining (s)  SWDR (W/m�)  PAR (�mol/m�/s)  \\\n",
              "date                                                                        \n",
              "2020-01-01 00:10:00        0.0          0.0          0.0              0.0   \n",
              "2020-01-01 00:20:00        0.0          0.0          0.0              0.0   \n",
              "2020-01-01 00:30:00        0.0          0.0          0.0              0.0   \n",
              "2020-01-01 00:40:00        0.0          0.0          0.0              0.0   \n",
              "2020-01-01 00:50:00        0.0          0.0          0.0              0.0   \n",
              "...                        ...          ...          ...              ...   \n",
              "2020-12-31 23:20:00        0.0          0.0          0.0              0.0   \n",
              "2020-12-31 23:30:00        0.0          0.0          0.0              0.0   \n",
              "2020-12-31 23:40:00        0.0          0.0          0.0              0.0   \n",
              "2020-12-31 23:50:00        0.0          0.0          0.0              0.0   \n",
              "2021-01-01 00:00:00        0.0          0.0          0.0              0.0   \n",
              "\n",
              "                     max. PAR (�mol/m�/s)  Tlog (degC)     OT  \n",
              "date                                                           \n",
              "2020-01-01 00:10:00                   0.0        11.45  428.1  \n",
              "2020-01-01 00:20:00                   0.0        11.51  428.0  \n",
              "2020-01-01 00:30:00                   0.0        11.60  427.6  \n",
              "2020-01-01 00:40:00                   0.0        11.70  430.0  \n",
              "2020-01-01 00:50:00                   0.0        11.81  432.2  \n",
              "...                                   ...          ...    ...  \n",
              "2020-12-31 23:20:00                   0.0        13.40  433.0  \n",
              "2020-12-31 23:30:00                   0.0        13.42  439.6  \n",
              "2020-12-31 23:40:00                   0.0        13.45  435.2  \n",
              "2020-12-31 23:50:00                   0.0        13.47  433.9  \n",
              "2021-01-01 00:00:00                   0.0        13.48  436.5  \n",
              "\n",
              "[52696 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d53ace28-801c-446b-954a-997103a23dbe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>p (mbar)</th>\n",
              "      <th>T (degC)</th>\n",
              "      <th>Tpot (K)</th>\n",
              "      <th>Tdew (degC)</th>\n",
              "      <th>rh (%)</th>\n",
              "      <th>VPmax (mbar)</th>\n",
              "      <th>VPact (mbar)</th>\n",
              "      <th>VPdef (mbar)</th>\n",
              "      <th>sh (g/kg)</th>\n",
              "      <th>H2OC (mmol/mol)</th>\n",
              "      <th>...</th>\n",
              "      <th>wv (m/s)</th>\n",
              "      <th>max. wv (m/s)</th>\n",
              "      <th>wd (deg)</th>\n",
              "      <th>rain (mm)</th>\n",
              "      <th>raining (s)</th>\n",
              "      <th>SWDR (W/m�)</th>\n",
              "      <th>PAR (�mol/m�/s)</th>\n",
              "      <th>max. PAR (�mol/m�/s)</th>\n",
              "      <th>Tlog (degC)</th>\n",
              "      <th>OT</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-01-01 00:10:00</th>\n",
              "      <td>1008.89</td>\n",
              "      <td>0.71</td>\n",
              "      <td>273.18</td>\n",
              "      <td>-1.33</td>\n",
              "      <td>86.1</td>\n",
              "      <td>6.43</td>\n",
              "      <td>5.54</td>\n",
              "      <td>0.89</td>\n",
              "      <td>3.42</td>\n",
              "      <td>5.49</td>\n",
              "      <td>...</td>\n",
              "      <td>1.02</td>\n",
              "      <td>1.60</td>\n",
              "      <td>224.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.45</td>\n",
              "      <td>428.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-01 00:20:00</th>\n",
              "      <td>1008.76</td>\n",
              "      <td>0.75</td>\n",
              "      <td>273.22</td>\n",
              "      <td>-1.44</td>\n",
              "      <td>85.2</td>\n",
              "      <td>6.45</td>\n",
              "      <td>5.49</td>\n",
              "      <td>0.95</td>\n",
              "      <td>3.39</td>\n",
              "      <td>5.45</td>\n",
              "      <td>...</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.84</td>\n",
              "      <td>206.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.51</td>\n",
              "      <td>428.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-01 00:30:00</th>\n",
              "      <td>1008.66</td>\n",
              "      <td>0.73</td>\n",
              "      <td>273.21</td>\n",
              "      <td>-1.48</td>\n",
              "      <td>85.1</td>\n",
              "      <td>6.44</td>\n",
              "      <td>5.48</td>\n",
              "      <td>0.96</td>\n",
              "      <td>3.39</td>\n",
              "      <td>5.43</td>\n",
              "      <td>...</td>\n",
              "      <td>0.61</td>\n",
              "      <td>1.48</td>\n",
              "      <td>197.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.60</td>\n",
              "      <td>427.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-01 00:40:00</th>\n",
              "      <td>1008.64</td>\n",
              "      <td>0.37</td>\n",
              "      <td>272.86</td>\n",
              "      <td>-1.64</td>\n",
              "      <td>86.3</td>\n",
              "      <td>6.27</td>\n",
              "      <td>5.41</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.35</td>\n",
              "      <td>5.37</td>\n",
              "      <td>...</td>\n",
              "      <td>1.11</td>\n",
              "      <td>1.48</td>\n",
              "      <td>206.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.70</td>\n",
              "      <td>430.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-01 00:50:00</th>\n",
              "      <td>1008.61</td>\n",
              "      <td>0.33</td>\n",
              "      <td>272.82</td>\n",
              "      <td>-1.50</td>\n",
              "      <td>87.4</td>\n",
              "      <td>6.26</td>\n",
              "      <td>5.47</td>\n",
              "      <td>0.79</td>\n",
              "      <td>3.38</td>\n",
              "      <td>5.42</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49</td>\n",
              "      <td>1.40</td>\n",
              "      <td>209.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.81</td>\n",
              "      <td>432.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-31 23:20:00</th>\n",
              "      <td>978.32</td>\n",
              "      <td>2.28</td>\n",
              "      <td>277.16</td>\n",
              "      <td>-0.80</td>\n",
              "      <td>80.0</td>\n",
              "      <td>7.20</td>\n",
              "      <td>5.76</td>\n",
              "      <td>1.44</td>\n",
              "      <td>3.67</td>\n",
              "      <td>5.89</td>\n",
              "      <td>...</td>\n",
              "      <td>0.73</td>\n",
              "      <td>1.40</td>\n",
              "      <td>180.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.40</td>\n",
              "      <td>433.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-31 23:30:00</th>\n",
              "      <td>978.30</td>\n",
              "      <td>2.13</td>\n",
              "      <td>277.01</td>\n",
              "      <td>-0.43</td>\n",
              "      <td>83.1</td>\n",
              "      <td>7.12</td>\n",
              "      <td>5.92</td>\n",
              "      <td>1.20</td>\n",
              "      <td>3.77</td>\n",
              "      <td>6.05</td>\n",
              "      <td>...</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.82</td>\n",
              "      <td>174.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.42</td>\n",
              "      <td>439.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-31 23:40:00</th>\n",
              "      <td>978.26</td>\n",
              "      <td>1.99</td>\n",
              "      <td>276.88</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>82.2</td>\n",
              "      <td>7.05</td>\n",
              "      <td>5.80</td>\n",
              "      <td>1.26</td>\n",
              "      <td>3.69</td>\n",
              "      <td>5.93</td>\n",
              "      <td>...</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.76</td>\n",
              "      <td>248.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.45</td>\n",
              "      <td>435.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-31 23:50:00</th>\n",
              "      <td>978.26</td>\n",
              "      <td>2.07</td>\n",
              "      <td>276.95</td>\n",
              "      <td>-0.77</td>\n",
              "      <td>81.4</td>\n",
              "      <td>7.09</td>\n",
              "      <td>5.77</td>\n",
              "      <td>1.32</td>\n",
              "      <td>3.68</td>\n",
              "      <td>5.90</td>\n",
              "      <td>...</td>\n",
              "      <td>0.57</td>\n",
              "      <td>1.07</td>\n",
              "      <td>196.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.47</td>\n",
              "      <td>433.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:00:00</th>\n",
              "      <td>978.24</td>\n",
              "      <td>2.01</td>\n",
              "      <td>276.89</td>\n",
              "      <td>-0.66</td>\n",
              "      <td>82.4</td>\n",
              "      <td>7.06</td>\n",
              "      <td>5.82</td>\n",
              "      <td>1.24</td>\n",
              "      <td>3.71</td>\n",
              "      <td>5.95</td>\n",
              "      <td>...</td>\n",
              "      <td>0.57</td>\n",
              "      <td>1.08</td>\n",
              "      <td>221.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.48</td>\n",
              "      <td>436.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>52696 rows × 21 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d53ace28-801c-446b-954a-997103a23dbe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d53ace28-801c-446b-954a-997103a23dbe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d53ace28-801c-446b-954a-997103a23dbe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a183b586-1dc5-40ec-b861-fbeab6a6d10d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a183b586-1dc5-40ec-b861-fbeab6a6d10d')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a183b586-1dc5-40ec-b861-fbeab6a6d10d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "mzH4Wk-nxxG4",
        "outputId": "eb9e6903-73bb-438d-c47e-ddca308a053f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           p (mbar)      T (degC)      Tpot (K)   Tdew (degC)        rh (%)  \\\n",
              "count  52696.000000  52696.000000  52696.000000  52696.000000  52696.000000   \n",
              "mean     989.989233     10.818241    284.796938      5.409105     72.487133   \n",
              "std        9.207149      7.468671      7.616995      5.956722     19.230260   \n",
              "min      955.580000     -6.440000    266.190000    -13.810000     21.160000   \n",
              "25%      984.800000      4.590000    278.550000      0.777500     58.820000   \n",
              "50%      990.920000     10.230000    284.320000      5.260000     75.400000   \n",
              "75%      995.930000     16.180000    290.260000      9.700000     87.900000   \n",
              "max     1020.070000     34.800000    309.130000     20.500000    100.000000   \n",
              "\n",
              "       VPmax (mbar)  VPact (mbar)  VPdef (mbar)     sh (g/kg)  \\\n",
              "count  52696.000000  52696.000000  52696.000000  52696.000000   \n",
              "mean      14.487046      9.676828      4.810131      6.111159   \n",
              "std        7.632960      4.023504      5.539320      2.561536   \n",
              "min        3.770000      2.090000      0.000000      1.300000   \n",
              "25%        8.480000      6.460000      1.170000      4.070000   \n",
              "50%       12.480000      8.890000      2.740000      5.610000   \n",
              "75%       18.420000     12.050000      6.440000      7.620000   \n",
              "max       55.670000     24.160000     42.100000     15.400000   \n",
              "\n",
              "       H2OC (mmol/mol)  ...      wv (m/s)  max. wv (m/s)      wd (deg)  \\\n",
              "count     52696.000000  ...  52696.000000   52696.000000  52696.000000   \n",
              "mean          9.782341  ...      1.995935       3.632807    176.850276   \n",
              "std           4.082684  ...     43.596931       2.462467     81.194840   \n",
              "min           2.090000  ...  -9999.000000       0.000000      0.000000   \n",
              "25%           6.530000  ...      1.000000       1.770000    141.100000   \n",
              "50%           8.990000  ...      1.770000       3.000000    195.500000   \n",
              "75%          12.200000  ...      2.920000       4.850000    226.700000   \n",
              "max          24.530000  ...     13.770000      22.900000    360.000000   \n",
              "\n",
              "          rain (mm)   raining (s)   SWDR (W/m�)  PAR (�mol/m�/s)  \\\n",
              "count  52696.000000  52696.000000  52696.000000     52696.000000   \n",
              "mean       0.011773     25.907469    131.331822       259.863438   \n",
              "std        0.123289    111.503420    215.735116       422.786515   \n",
              "min        0.000000      0.000000      0.000000         0.000000   \n",
              "25%        0.000000      0.000000      0.000000         0.000000   \n",
              "50%        0.000000      0.000000      2.820000         9.670000   \n",
              "75%        0.000000      0.000000    182.900000       366.437500   \n",
              "max       11.200000    600.000000   1115.290000      2131.760000   \n",
              "\n",
              "       max. PAR (�mol/m�/s)   Tlog (degC)            OT  \n",
              "count          52696.000000  52696.000000  52696.000000  \n",
              "mean             308.601939     21.515206    417.798615  \n",
              "std              569.956762      7.790620    321.570015  \n",
              "min            -9999.000000      6.900000  -9999.000000  \n",
              "25%                0.000000     15.240000    415.500000  \n",
              "50%               13.120000     20.440000    423.200000  \n",
              "75%              434.260000     26.822500    437.100000  \n",
              "max             2498.940000     49.090000    524.200000  \n",
              "\n",
              "[8 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dee0a4ca-b48a-4a27-9bb2-28720b5ce92a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>p (mbar)</th>\n",
              "      <th>T (degC)</th>\n",
              "      <th>Tpot (K)</th>\n",
              "      <th>Tdew (degC)</th>\n",
              "      <th>rh (%)</th>\n",
              "      <th>VPmax (mbar)</th>\n",
              "      <th>VPact (mbar)</th>\n",
              "      <th>VPdef (mbar)</th>\n",
              "      <th>sh (g/kg)</th>\n",
              "      <th>H2OC (mmol/mol)</th>\n",
              "      <th>...</th>\n",
              "      <th>wv (m/s)</th>\n",
              "      <th>max. wv (m/s)</th>\n",
              "      <th>wd (deg)</th>\n",
              "      <th>rain (mm)</th>\n",
              "      <th>raining (s)</th>\n",
              "      <th>SWDR (W/m�)</th>\n",
              "      <th>PAR (�mol/m�/s)</th>\n",
              "      <th>max. PAR (�mol/m�/s)</th>\n",
              "      <th>Tlog (degC)</th>\n",
              "      <th>OT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "      <td>52696.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>989.989233</td>\n",
              "      <td>10.818241</td>\n",
              "      <td>284.796938</td>\n",
              "      <td>5.409105</td>\n",
              "      <td>72.487133</td>\n",
              "      <td>14.487046</td>\n",
              "      <td>9.676828</td>\n",
              "      <td>4.810131</td>\n",
              "      <td>6.111159</td>\n",
              "      <td>9.782341</td>\n",
              "      <td>...</td>\n",
              "      <td>1.995935</td>\n",
              "      <td>3.632807</td>\n",
              "      <td>176.850276</td>\n",
              "      <td>0.011773</td>\n",
              "      <td>25.907469</td>\n",
              "      <td>131.331822</td>\n",
              "      <td>259.863438</td>\n",
              "      <td>308.601939</td>\n",
              "      <td>21.515206</td>\n",
              "      <td>417.798615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>9.207149</td>\n",
              "      <td>7.468671</td>\n",
              "      <td>7.616995</td>\n",
              "      <td>5.956722</td>\n",
              "      <td>19.230260</td>\n",
              "      <td>7.632960</td>\n",
              "      <td>4.023504</td>\n",
              "      <td>5.539320</td>\n",
              "      <td>2.561536</td>\n",
              "      <td>4.082684</td>\n",
              "      <td>...</td>\n",
              "      <td>43.596931</td>\n",
              "      <td>2.462467</td>\n",
              "      <td>81.194840</td>\n",
              "      <td>0.123289</td>\n",
              "      <td>111.503420</td>\n",
              "      <td>215.735116</td>\n",
              "      <td>422.786515</td>\n",
              "      <td>569.956762</td>\n",
              "      <td>7.790620</td>\n",
              "      <td>321.570015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>955.580000</td>\n",
              "      <td>-6.440000</td>\n",
              "      <td>266.190000</td>\n",
              "      <td>-13.810000</td>\n",
              "      <td>21.160000</td>\n",
              "      <td>3.770000</td>\n",
              "      <td>2.090000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.300000</td>\n",
              "      <td>2.090000</td>\n",
              "      <td>...</td>\n",
              "      <td>-9999.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9999.000000</td>\n",
              "      <td>6.900000</td>\n",
              "      <td>-9999.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>984.800000</td>\n",
              "      <td>4.590000</td>\n",
              "      <td>278.550000</td>\n",
              "      <td>0.777500</td>\n",
              "      <td>58.820000</td>\n",
              "      <td>8.480000</td>\n",
              "      <td>6.460000</td>\n",
              "      <td>1.170000</td>\n",
              "      <td>4.070000</td>\n",
              "      <td>6.530000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.770000</td>\n",
              "      <td>141.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.240000</td>\n",
              "      <td>415.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>990.920000</td>\n",
              "      <td>10.230000</td>\n",
              "      <td>284.320000</td>\n",
              "      <td>5.260000</td>\n",
              "      <td>75.400000</td>\n",
              "      <td>12.480000</td>\n",
              "      <td>8.890000</td>\n",
              "      <td>2.740000</td>\n",
              "      <td>5.610000</td>\n",
              "      <td>8.990000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.770000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>195.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.820000</td>\n",
              "      <td>9.670000</td>\n",
              "      <td>13.120000</td>\n",
              "      <td>20.440000</td>\n",
              "      <td>423.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>995.930000</td>\n",
              "      <td>16.180000</td>\n",
              "      <td>290.260000</td>\n",
              "      <td>9.700000</td>\n",
              "      <td>87.900000</td>\n",
              "      <td>18.420000</td>\n",
              "      <td>12.050000</td>\n",
              "      <td>6.440000</td>\n",
              "      <td>7.620000</td>\n",
              "      <td>12.200000</td>\n",
              "      <td>...</td>\n",
              "      <td>2.920000</td>\n",
              "      <td>4.850000</td>\n",
              "      <td>226.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>182.900000</td>\n",
              "      <td>366.437500</td>\n",
              "      <td>434.260000</td>\n",
              "      <td>26.822500</td>\n",
              "      <td>437.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1020.070000</td>\n",
              "      <td>34.800000</td>\n",
              "      <td>309.130000</td>\n",
              "      <td>20.500000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>55.670000</td>\n",
              "      <td>24.160000</td>\n",
              "      <td>42.100000</td>\n",
              "      <td>15.400000</td>\n",
              "      <td>24.530000</td>\n",
              "      <td>...</td>\n",
              "      <td>13.770000</td>\n",
              "      <td>22.900000</td>\n",
              "      <td>360.000000</td>\n",
              "      <td>11.200000</td>\n",
              "      <td>600.000000</td>\n",
              "      <td>1115.290000</td>\n",
              "      <td>2131.760000</td>\n",
              "      <td>2498.940000</td>\n",
              "      <td>49.090000</td>\n",
              "      <td>524.200000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 21 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dee0a4ca-b48a-4a27-9bb2-28720b5ce92a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dee0a4ca-b48a-4a27-9bb2-28720b5ce92a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dee0a4ca-b48a-4a27-9bb2-28720b5ce92a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a7a8f7d5-87a4-4394-bfe5-1c1cbed96afb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a7a8f7d5-87a4-4394-bfe5-1c1cbed96afb')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a7a8f7d5-87a4-4394-bfe5-1c1cbed96afb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "mgVd2Yt6x3L8",
        "outputId": "fb989f5e-0bab-48b0-83b3-ad9737d79a05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 52696 entries, 2020-01-01 00:10:00 to 2021-01-01 00:00:00\n",
            "Data columns (total 21 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   p (mbar)              52696 non-null  float64\n",
            " 1   T (degC)              52696 non-null  float64\n",
            " 2   Tpot (K)              52696 non-null  float64\n",
            " 3   Tdew (degC)           52696 non-null  float64\n",
            " 4   rh (%)                52696 non-null  float64\n",
            " 5   VPmax (mbar)          52696 non-null  float64\n",
            " 6   VPact (mbar)          52696 non-null  float64\n",
            " 7   VPdef (mbar)          52696 non-null  float64\n",
            " 8   sh (g/kg)             52696 non-null  float64\n",
            " 9   H2OC (mmol/mol)       52696 non-null  float64\n",
            " 10  rho (g/m**3)          52696 non-null  float64\n",
            " 11  wv (m/s)              52696 non-null  float64\n",
            " 12  max. wv (m/s)         52696 non-null  float64\n",
            " 13  wd (deg)              52696 non-null  float64\n",
            " 14  rain (mm)             52696 non-null  float64\n",
            " 15  raining (s)           52696 non-null  float64\n",
            " 16  SWDR (W/m�)           52696 non-null  float64\n",
            " 17  PAR (�mol/m�/s)       52696 non-null  float64\n",
            " 18  max. PAR (�mol/m�/s)  52696 non-null  float64\n",
            " 19  Tlog (degC)           52696 non-null  float64\n",
            " 20  OT                    52696 non-null  float64\n",
            "dtypes: float64(21)\n",
            "memory usage: 8.8 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P7MY7_-Ax4Up"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}